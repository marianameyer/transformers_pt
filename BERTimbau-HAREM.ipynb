{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/jsansao/transformers_pt/blob/main/BERTimbau_Fine_tuning_HAREM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github"}},{"cell_type":"markdown","source":"# Inferência usando transformers pré-treinados do HuggingFace ","metadata":{"id":"hV_s2jhFIGRV"}},{"cell_type":"markdown","source":"BERTimbau\n\nHAREM\n\nBatch size = 16\n\nGPU T4x2","metadata":{}},{"cell_type":"code","source":"#@title Passo 1:Instalando Hugging Face Transformers\n# We won't need TensorFlow here\n!pip uninstall -y tensorflow\n# Install `transformers` from master\n!pip install transformers datasets\n!pip list | grep -E 'transformers|tokenizers'\n# transformers version at notebook update --- 2.9.1\n# tokenizers version at notebook update --- 0.7.0","metadata":{"id":"LhI1tJBGBd3r","outputId":"e93521b4-540f-4217-886a-8b9f649672e3","execution":{"iopub.status.busy":"2022-11-07T17:46:42.108740Z","iopub.execute_input":"2022-11-07T17:46:42.109123Z","iopub.status.idle":"2022-11-07T17:47:18.004867Z","shell.execute_reply.started":"2022-11-07T17:46:42.109091Z","shell.execute_reply":"2022-11-07T17:47:18.003579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset, load_metric\n\ndataset = load_dataset(\"harem\")\n#dataset = load_dataset(\"wnut_17\")","metadata":{"id":"KAl5BxxOgk35","outputId":"4591ed73-b0ad-47f3-c568-2335bd874a17","execution":{"iopub.status.busy":"2022-11-07T17:47:18.008790Z","iopub.execute_input":"2022-11-07T17:47:18.009142Z","iopub.status.idle":"2022-11-07T17:47:22.560602Z","shell.execute_reply.started":"2022-11-07T17:47:18.009108Z","shell.execute_reply":"2022-11-07T17:47:22.559577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Passo 2:Baixando e salvando BR_BERTo\n#https://huggingface.co/rdenadai/BR_BERTo\n\nfrom transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments\n\n#tokenizer = AutoTokenizer.from_pretrained(\"rdenadai/BR_BERTo\")\n#model = AutoModelForMaskedLM.from_pretrained(\"rdenadai/BR_BERTo\")\n\n#tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n#model = AutoModelForTokenClassification.from_pretrained('neuralmind/bert-base-portuguese-cased')\n\nmodel_checkpoint = \"neuralmind/bert-base-portuguese-cased\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{"id":"h2L_Put8Cn8S","outputId":"0189aac4-0f58-4ae3-f126-1f39fa9e646e","execution":{"iopub.status.busy":"2022-11-07T17:47:22.562485Z","iopub.execute_input":"2022-11-07T17:47:22.563167Z","iopub.status.idle":"2022-11-07T17:47:27.702968Z","shell.execute_reply.started":"2022-11-07T17:47:22.563127Z","shell.execute_reply":"2022-11-07T17:47:27.702005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[\"train\"][0]","metadata":{"id":"J1oYngljZeZT","outputId":"8ee1841c-ae4c-4dd2-b74f-605d349c7cb3","execution":{"iopub.status.busy":"2022-11-07T17:47:27.705672Z","iopub.execute_input":"2022-11-07T17:47:27.706030Z","iopub.status.idle":"2022-11-07T17:47:27.723767Z","shell.execute_reply.started":"2022-11-07T17:47:27.705993Z","shell.execute_reply":"2022-11-07T17:47:27.722693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"label_list = dataset[\"train\"].features[f\"ner_tags\"].feature.names\nlabel_list","metadata":{"id":"LiZZQgAK1kU4","outputId":"863034c0-a447-46fb-8b87-136475ee0d12","execution":{"iopub.status.busy":"2022-11-07T17:47:27.725105Z","iopub.execute_input":"2022-11-07T17:47:27.725803Z","iopub.status.idle":"2022-11-07T17:47:27.747356Z","shell.execute_reply.started":"2022-11-07T17:47:27.725767Z","shell.execute_reply":"2022-11-07T17:47:27.738699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"exemplo de entrada","metadata":{"id":"ETLIt8siKjWC"}},{"cell_type":"code","source":"label_all_tokens = True","metadata":{"id":"qkiSndhDKHCg","execution":{"iopub.status.busy":"2022-11-07T17:47:27.748635Z","iopub.execute_input":"2022-11-07T17:47:27.749036Z","iopub.status.idle":"2022-11-07T17:47:27.754744Z","shell.execute_reply.started":"2022-11-07T17:47:27.748997Z","shell.execute_reply":"2022-11-07T17:47:27.753430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_and_align_labels(examples):\n    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True, max_length=512)\n\n    labels = []\n    for i, label in enumerate(examples[f\"ner_tags\"]):\n        word_ids = tokenized_inputs.word_ids(batch_index=i)\n        previous_word_idx = None\n        label_ids = []\n        for word_idx in word_ids:\n            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n            # ignored in the loss function.\n            if word_idx is None:\n                label_ids.append(-100)\n            # We set the label for the first token of each word.\n            elif word_idx != previous_word_idx:\n                label_ids.append(label[word_idx])\n            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n            # the label_all_tokens flag.\n            else:\n                label_ids.append(label[word_idx] if label_all_tokens else -100)\n            previous_word_idx = word_idx\n\n        labels.append(label_ids)\n\n    tokenized_inputs[\"labels\"] = labels\n    return tokenized_inputs","metadata":{"id":"KHSGe5PiJ6VQ","execution":{"iopub.status.busy":"2022-11-07T17:47:27.756469Z","iopub.execute_input":"2022-11-07T17:47:27.757466Z","iopub.status.idle":"2022-11-07T17:47:27.770897Z","shell.execute_reply.started":"2022-11-07T17:47:27.757422Z","shell.execute_reply":"2022-11-07T17:47:27.769130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)","metadata":{"id":"jIA6C28QHUY7","outputId":"4fb159cc-0c09-4939-80a9-39704616544d","execution":{"iopub.status.busy":"2022-11-07T17:47:27.773309Z","iopub.execute_input":"2022-11-07T17:47:27.774569Z","iopub.status.idle":"2022-11-07T17:47:29.588685Z","shell.execute_reply.started":"2022-11-07T17:47:27.774515Z","shell.execute_reply":"2022-11-07T17:47:29.587827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Step 11: Defining a Data Collator\nfrom transformers import DataCollatorForTokenClassification\n\ndata_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n\nmodel = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))","metadata":{"id":"19hRhnHFf7FF","outputId":"4f6cc49c-8967-4aa9-b1dc-41ca2033162d","execution":{"iopub.status.busy":"2022-11-07T17:47:29.592808Z","iopub.execute_input":"2022-11-07T17:47:29.593500Z","iopub.status.idle":"2022-11-07T17:47:48.436349Z","shell.execute_reply.started":"2022-11-07T17:47:29.593461Z","shell.execute_reply":"2022-11-07T17:47:48.435161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install seqeval","metadata":{"id":"X4HzKUKfH33t","outputId":"502524ac-2c6f-453e-af8f-cc9c06c21bff","execution":{"iopub.status.busy":"2022-11-07T17:47:48.441411Z","iopub.execute_input":"2022-11-07T17:47:48.441740Z","iopub.status.idle":"2022-11-07T17:48:02.713317Z","shell.execute_reply.started":"2022-11-07T17:47:48.441707Z","shell.execute_reply":"2022-11-07T17:48:02.712025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir=\"./BERTimbau-HAREM\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=20,\n    weight_decay=0.01,\n)","metadata":{"id":"MH2dsTzhfgqI","outputId":"5527cccf-74ee-42d1-f9aa-005059b836ce","execution":{"iopub.status.busy":"2022-11-07T17:48:02.715135Z","iopub.execute_input":"2022-11-07T17:48:02.715543Z","iopub.status.idle":"2022-11-07T17:48:03.021910Z","shell.execute_reply.started":"2022-11-07T17:48:02.715498Z","shell.execute_reply":"2022-11-07T17:48:03.020772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"example = dataset[\"train\"][0]\n\nmetric = load_metric(\"seqeval\")\n\nlabels = [label_list[i] for i in example[f\"ner_tags\"]]\nmetric.compute(predictions=[labels], references=[labels])","metadata":{"id":"CPcOrwRKHt5A","outputId":"71a9ca77-d30c-4378-c0c0-4a6e6f8e77bf","execution":{"iopub.status.busy":"2022-11-07T17:48:03.023721Z","iopub.execute_input":"2022-11-07T17:48:03.025239Z","iopub.status.idle":"2022-11-07T17:48:03.571986Z","shell.execute_reply.started":"2022-11-07T17:48:03.025194Z","shell.execute_reply":"2022-11-07T17:48:03.570907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef compute_metrics(p):\n    predictions, labels = p\n    predictions = np.argmax(predictions, axis=2)\n\n    # Remove ignored index (special tokens)\n    true_predictions = [\n        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n    true_labels = [\n        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n        for prediction, label in zip(predictions, labels)\n    ]\n\n    results = metric.compute(predictions=true_predictions, references=true_labels)\n    return {\n        \"precision\": results[\"overall_precision\"],\n        \"recall\": results[\"overall_recall\"],\n        \"f1\": results[\"overall_f1\"],\n        \"accuracy\": results[\"overall_accuracy\"],\n    }","metadata":{"id":"DQ8QTg-KIptS","execution":{"iopub.status.busy":"2022-11-07T17:48:03.573522Z","iopub.execute_input":"2022-11-07T17:48:03.574029Z","iopub.status.idle":"2022-11-07T17:48:03.583474Z","shell.execute_reply.started":"2022-11-07T17:48:03.573988Z","shell.execute_reply":"2022-11-07T17:48:03.581781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)","metadata":{"id":"J-6RJoG-Io4l","execution":{"iopub.status.busy":"2022-11-07T17:48:03.585288Z","iopub.execute_input":"2022-11-07T17:48:03.586100Z","iopub.status.idle":"2022-11-07T17:48:07.131930Z","shell.execute_reply.started":"2022-11-07T17:48:03.586000Z","shell.execute_reply":"2022-11-07T17:48:07.130726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset","metadata":{"id":"SEs2xZrlMk-G","outputId":"fc60345c-ae8c-4f6f-8de1-28fdaa0db3bf","execution":{"iopub.status.busy":"2022-11-07T17:48:07.133888Z","iopub.execute_input":"2022-11-07T17:48:07.134313Z","iopub.status.idle":"2022-11-07T17:48:07.145337Z","shell.execute_reply.started":"2022-11-07T17:48:07.134271Z","shell.execute_reply":"2022-11-07T17:48:07.144392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_dataset","metadata":{"id":"3UNOhFBHNUre","outputId":"b1d15593-ef5d-4da3-fe78-04056d2a70c9","execution":{"iopub.status.busy":"2022-11-07T17:48:07.146965Z","iopub.execute_input":"2022-11-07T17:48:07.147622Z","iopub.status.idle":"2022-11-07T17:48:07.213255Z","shell.execute_reply.started":"2022-11-07T17:48:07.147573Z","shell.execute_reply":"2022-11-07T17:48:07.212315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Step 13: Pre-training the Model\n\ntrainer.train()","metadata":{"id":"EZxksqadfv6l","outputId":"bcc22f4a-364a-4c16-dab0-b14ee88d1723","execution":{"iopub.status.busy":"2022-11-07T17:48:07.214381Z","iopub.execute_input":"2022-11-07T17:48:07.214689Z","iopub.status.idle":"2022-11-07T17:52:10.505168Z","shell.execute_reply.started":"2022-11-07T17:48:07.214662Z","shell.execute_reply":"2022-11-07T17:52:10.504035Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Step 14: Saving the Final Model(+tokenizer + config) to disk\n#trainer.save_model(\"./KantaiBERT\")\ntrainer.save_model('./BERTimbau-HAREM')","metadata":{"execution":{"iopub.status.busy":"2022-11-07T17:52:10.506853Z","iopub.execute_input":"2022-11-07T17:52:10.507873Z","iopub.status.idle":"2022-11-07T17:52:12.011899Z","shell.execute_reply.started":"2022-11-07T17:52:10.507823Z","shell.execute_reply":"2022-11-07T17:52:12.010905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving model on Wandb\nimport wandb\nwandb.save('BERTimbau-HAREM.h5')","metadata":{"execution":{"iopub.status.busy":"2022-11-07T17:52:12.016611Z","iopub.execute_input":"2022-11-07T17:52:12.022962Z","iopub.status.idle":"2022-11-07T17:52:12.039235Z","shell.execute_reply.started":"2022-11-07T17:52:12.022916Z","shell.execute_reply":"2022-11-07T17:52:12.037201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\neval_results = trainer.evaluate()\nprint(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")","metadata":{"id":"p9rpjGVyj68j","outputId":"f5fb6d3b-ab9e-40dd-96a9-38ead28ef831","execution":{"iopub.status.busy":"2022-11-07T17:52:12.040830Z","iopub.execute_input":"2022-11-07T17:52:12.041154Z","iopub.status.idle":"2022-11-07T17:52:16.258196Z","shell.execute_reply.started":"2022-11-07T17:52:12.041118Z","shell.execute_reply":"2022-11-07T17:52:16.257301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eval_results","metadata":{"id":"Ihaz7TAMAF4l","outputId":"40dd93a8-fd4f-4a5f-e4dd-5f0cbea693fb","execution":{"iopub.status.busy":"2022-11-07T17:52:16.260272Z","iopub.execute_input":"2022-11-07T17:52:16.260568Z","iopub.status.idle":"2022-11-07T17:52:16.269225Z","shell.execute_reply.started":"2022-11-07T17:52:16.260528Z","shell.execute_reply":"2022-11-07T17:52:16.268258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Step 14: Saving the Final Model(+tokenizer + config) to disk\ntrainer.save_model(\"./BERTimbau-HAREM\")","metadata":{"id":"xusSgo3unkRI","outputId":"026f347c-98eb-467a-de3f-515f424fdc74","execution":{"iopub.status.busy":"2022-11-07T17:52:16.271090Z","iopub.execute_input":"2022-11-07T17:52:16.271846Z","iopub.status.idle":"2022-11-07T17:52:17.690905Z","shell.execute_reply.started":"2022-11-07T17:52:16.271791Z","shell.execute_reply":"2022-11-07T17:52:17.689920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#@title Step 3: Configurando o pipeline fill-mask\n#@title Step 15: Language Modeling with the FillMaskPipeline\nfrom transformers import pipeline\n\nclassifier = pipeline(\"token-classification\", model = \"./BERTimbau-HAREM\", tokenizer=tokenizer)","metadata":{"id":"pqy7oTgYFb9Y","outputId":"ca6d684a-7cb3-4fe2-d299-06e4a0d682e0","execution":{"iopub.status.busy":"2022-11-07T17:52:17.692461Z","iopub.execute_input":"2022-11-07T17:52:17.692852Z","iopub.status.idle":"2022-11-07T17:52:19.523582Z","shell.execute_reply.started":"2022-11-07T17:52:17.692815Z","shell.execute_reply":"2022-11-07T17:52:19.522599Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier(\"Apple ou Google\")","metadata":{"id":"_RYDjccHPjYB","outputId":"f40aac25-7efe-465d-a033-3456852a065e","execution":{"iopub.status.busy":"2022-11-07T17:52:19.525077Z","iopub.execute_input":"2022-11-07T17:52:19.525433Z","iopub.status.idle":"2022-11-07T17:52:19.773813Z","shell.execute_reply.started":"2022-11-07T17:52:19.525395Z","shell.execute_reply":"2022-11-07T17:52:19.772821Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier(\"ele se chama Carlos Alberto. Mora em Juiz de Fora. Trabalha no Carrefour. Seu aniversário é dia 20 de outubro.\")","metadata":{"id":"-rq4EYjTQWKI","outputId":"8c5f5f3e-cbe9-4fe9-b8d6-e040fe4fd05d","execution":{"iopub.status.busy":"2022-11-07T17:52:19.775133Z","iopub.execute_input":"2022-11-07T17:52:19.775798Z","iopub.status.idle":"2022-11-07T17:52:19.923773Z","shell.execute_reply.started":"2022-11-07T17:52:19.775758Z","shell.execute_reply":"2022-11-07T17:52:19.922835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"classifier(\"A Huawei é uma multinacional sediada em Shenzhen, China.\")","metadata":{"execution":{"iopub.status.busy":"2022-11-07T17:52:19.925185Z","iopub.execute_input":"2022-11-07T17:52:19.925673Z","iopub.status.idle":"2022-11-07T17:52:20.054063Z","shell.execute_reply.started":"2022-11-07T17:52:19.925635Z","shell.execute_reply":"2022-11-07T17:52:20.053069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2022-11-07T17:52:20.055465Z","iopub.execute_input":"2022-11-07T17:52:20.056481Z","iopub.status.idle":"2022-11-07T17:52:24.197581Z","shell.execute_reply.started":"2022-11-07T17:52:20.056441Z","shell.execute_reply":"2022-11-07T17:52:24.196731Z"},"trusted":true},"execution_count":null,"outputs":[]}]}